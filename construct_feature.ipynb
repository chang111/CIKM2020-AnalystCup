{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 由于favorite是目前比较有用的feature。这里需要总结几个目前想到的特征: \n",
    "1. 根据entiti去总结出被引用最多的50种entitie，然后算他们出现时的retweet\n",
    "2. 根据hashtage总结出引用最多的50种hashtag，然后他们出现时的retweet\n",
    "3. 根据favorite筛选出其中的最大值以及最小值\n",
    "4. 根据favorite选出最常搭配的10个entitie，10个hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invite people for the Kaggle party\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = '/home/chai/Projects/CIKM2020'\n",
    "INPUT_PATH = os.path.join(ROOT_PATH, 'public_dat_50027')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train data and train solution\n",
    "import time\n",
    "X_train_list = []\n",
    "with open(os.path.join(INPUT_PATH, 'train.data'), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        X_train_list.append(line.strip('\\n').split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the solution\n",
    "y_train_list = []\n",
    "with open(os.path.join(INPUT_PATH, 'train.solution'), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        y_train_list.append([line.strip('\\n')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the column\n",
    "feature_name = pd.read_table(os.path.join(INPUT_PATH, 'feature.name'), sep='\\t')\n",
    "train_list = []\n",
    "for i, item in enumerate(X_train_list):\n",
    "    if np.random.random() < 0.01:\n",
    "        train_list.append(item + [int(y_train_list[i][0])])\n",
    "#     if i > 50000:\n",
    "#         break\n",
    "#     if item[5] != '0':\n",
    "#         item.append(solution[i][0])\n",
    "#         train.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('采样率为: ', len(train_list) / len(X_train_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_list, y_train_list\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(train_list, columns=feature_name.columns.tolist() + ['solution'])\n",
    "\n",
    "train_data['weekday'] = train_data['timestamp'].map(lambda x: x.split(\" \")[0])\n",
    "train_data['month'] = train_data['timestamp'].map(lambda x: x.split(\" \")[1])\n",
    "train_data['day'] = train_data['timestamp'].map(lambda x: int(x.split(\" \")[2]))\n",
    "train_data['hour'] = train_data['timestamp'].map(lambda x: int(x.split(\" \")[3].split(\":\")[0]))\n",
    "train_data['year'] = train_data['timestamp'].map(lambda x: int(x.split(\" \")[-1]))\n",
    "week_day = {\"Mon\": 1,\"Tue\":2,\"Wed\":3,\"Thu\":4,\"Fri\":5,\"Sat\":6,\"Sun\":7}\n",
    "month_day = {\"Sep\":9,\"Oct\":10,\"Nov\":11,\"Dec\":12,\"Jan\":1,\"Feb\":2,\"Mar\":3,\"Apr\":4}\n",
    "train_data['weekday'] = train_data['weekday'].replace(week_day)\n",
    "train_data['month'] = train_data['month'].replace(month_day)\n",
    "# train_data = train_data.drop(columns=['timestamp'])\n",
    "\n",
    "train_data['#followers'] = train_data['#followers'].astype(int)\n",
    "train_data['#friends'] = train_data['#friends'].astype(int)\n",
    "train_data['#favorites'] = train_data['#favorites'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# favorites为0，但是solution不为0的个数\n",
    "df0 = train_data.loc[train_data['#favorites'] == 0]\n",
    "df1 = df0.loc[df0['solution'] > 0]\n",
    "print('favorites为0，但是solution不为0的个数: ', df1.shape[0], ', 比例为: ', df1.shape[0] / df0.shape[0])\n",
    "df0[['#followers', '#friends', '#favorites', 'solution']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total: ', len(train_data))\n",
    "train_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 10000 * train_data['year'] + 100 * train_data['month'] + train_data['day']\n",
    "print('最早的时间:', df.min(), ', 最晚的时间:', df.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_day = datetime.strptime('2019-09-30', '%Y-%m-%d')\n",
    "train_data['timestamp'] = train_data['timestamp'].map(lambda x : (datetime.strptime(x, '%a %b %d %H:%M:%S +0000 %Y') - start_day).days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sentiment_left'] = train_data['sentiment'].map(lambda x : int(x.split(' ')[0]))\n",
    "train_data['sentiment_right'] = train_data['sentiment'].map(lambda x : int(x.split(' ')[1]))\n",
    "train_data['sentiment_sum'] = train_data['sentiment_left'] + train_data['sentiment_right']\n",
    "train_data['sentiment_sumabs'] = (train_data['sentiment_left'] + train_data['sentiment_right']).map(lambda x : abs(x))\n",
    "train_data['sentiment_diff'] = train_data['sentiment_left'] - train_data['sentiment_right']\n",
    "train_data['sentiment_div'] = train_data['sentiment_left'] / train_data['sentiment_right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for var in ['sentiment_left', 'sentiment_right', 'sentiment_sum', 'sentiment_sumabs', 'sentiment_diff', 'sentiment_div']:\n",
    "    print(var)\n",
    "    print(train_data[var].value_counts())\n",
    "    data = pd.concat([train_data['solution'], train_data[var]], axis=1)\n",
    "    f, ax = plt.subplots(figsize=(8, 6))\n",
    "    fig = sns.boxplot(x=var, y=\"solution\", data=data)\n",
    "    fig.axis(ymin=0, ymax=40);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#saleprice correlation matrix\n",
    "corrmat = train_data[['solution', 'sentiment_left', 'sentiment_right', 'sentiment_sum', 'sentiment_sumabs', 'sentiment_diff', 'sentiment_div']].corr()\n",
    "k = 10 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'solution')['solution'].index\n",
    "cm = np.corrcoef(train_data[cols].values.T)\n",
    "f, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[train_data['mentions'] != 'null;']['mentions'].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('特征mentions不为null;的值有:', train_data.loc[train_data['mentions'] != 'null;'].shape[0])\n",
    "print('比例为:', train_data.loc[train_data['mentions'] != 'null;'].shape[0] / train_data.shape[0])\n",
    "print('特征mentions的数量为:', train_data['mentions'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[train_data['entities'] != 'null;']['entities'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list = []\n",
    "for each_entity in train_data['entities']:\n",
    "    if each_entity != 'null;':\n",
    "        for each in each_entity.strip(';').split(';'):\n",
    "            entities_list.append(float(each.split(':')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('特征entities不为null;的值有:', train_data.loc[train_data['entities'] != 'null;'].shape[0])\n",
    "print('比例为:', train_data.loc[train_data['entities'] != 'null;'].shape[0] / train_data.shape[0])\n",
    "print('特征entities的数量为:', train_data['entities'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('特征entities的最大值:', max(entities_list))\n",
    "print('特征entities的最小值:', min(entities_list))\n",
    "print('特征entities的均值:', np.mean(entities_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_mean = []\n",
    "for each_entity in train_data['entities']:\n",
    "    if each_entity != 'null;':\n",
    "        tmp = []\n",
    "        for each in each_entity.strip(';').split(';'):\n",
    "            tmp.append(float(each.split(':')[-1]))\n",
    "        entities_mean.append(np.mean(tmp))\n",
    "    else:\n",
    "        entities_mean.append(0)\n",
    "train_data['entities_mean'] = entities_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['entities_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_max = []\n",
    "for each_entity in train_data['entities']:\n",
    "    if each_entity != 'null;':\n",
    "        tmp = []\n",
    "        for each in each_entity.strip(';').split(';'):\n",
    "            tmp.append(float(each.split(':')[-1]))\n",
    "        entities_max.append(max(tmp))\n",
    "    else:\n",
    "        entities_max.append(0)\n",
    "train_data['entities_max'] = entities_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_min = []\n",
    "for each_entity in train_data['entities']:\n",
    "    if each_entity != 'null;':\n",
    "        tmp = []\n",
    "        for each in each_entity.strip(';').split(';'):\n",
    "            tmp.append(float(each.split(':')[-1]))\n",
    "        entities_min.append(max(tmp))\n",
    "    else:\n",
    "        entities_min.append(0)\n",
    "train_data['entities_min'] = entities_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_std = []\n",
    "for each_entity in train_data['entities']:\n",
    "    if each_entity != 'null;':\n",
    "        tmp = []\n",
    "        for each in each_entity.strip(';').split(';'):\n",
    "            tmp.append(float(each.split(':')[-1]))\n",
    "        entities_std.append(np.std(tmp))\n",
    "    else:\n",
    "        entities_std.append(0)\n",
    "train_data['entities_std'] = entities_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_len = []\n",
    "for each_entity in train_data['entities']:\n",
    "    if each_entity != 'null;':\n",
    "        entities_len.append(len(each_entity.strip(';').split(';')))\n",
    "    else:\n",
    "        entities_len.append(0)\n",
    "train_data['entities_len'] = entities_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saleprice correlation matrix\n",
    "corrmat = train_data[['solution', 'entities_mean', 'entities_max', 'entities_min', 'entities_std', 'entities_len']].corr()\n",
    "k = 10 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'solution')['solution'].index\n",
    "cm = np.corrcoef(train_data[cols].values.T)\n",
    "f, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('特征mentions不为null;的值有:', train_data.loc[train_data['mentions'] != 'null;'].shape[0])\n",
    "print('比例为:', train_data.loc[train_data['mentions'] != 'null;'].shape[0] / train_data.shape[0])\n",
    "print('特征mentions的数量为:', train_data['mentions'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['mentions'].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_count_dict = {}\n",
    "for each_mention in train_data['mentions']:\n",
    "    if each_mention != 'null;':\n",
    "        for each in each_mention.strip().split(' '):\n",
    "            mentions_count_dict[each] = mentions_count_dict.get(each, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_len = []\n",
    "for each_mention in train_data['mentions']:\n",
    "    if each_mention != 'null;':\n",
    "        mentions_len.append(len(each_mention.split(' ')))\n",
    "    else:\n",
    "        mentions_len.append(0)\n",
    "train_data['mentions_len'] = mentions_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_value= []\n",
    "for each_mention in train_data['mentions']:\n",
    "    if each_mention != 'null;':\n",
    "        tmp = 0\n",
    "        for each in each_mention.strip().split(' '):\n",
    "            tmp += mentions_count_dict[each]\n",
    "        mentions_value.append(tmp)\n",
    "    else:\n",
    "        mentions_value.append(0)\n",
    "train_data['mentions_value'] = mentions_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saleprice correlation matrix\n",
    "corrmat = train_data[['solution', 'mentions_len', 'mentions_value']].corr()\n",
    "k = 10 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'solution')['solution'].index\n",
    "cm = np.corrcoef(train_data[cols].values.T)\n",
    "f, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('特征hashtags不为null;的值有:', train_data.loc[train_data['hashtags'] != 'null;'].shape[0])\n",
    "print('比例为:', train_data.loc[train_data['hashtags'] != 'null;'].shape[0] / train_data.shape[0])\n",
    "print('特征hashtags的数量为:', train_data['hashtags'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_count_dict = {}\n",
    "for each_hashtag in train_data['hashtags']:\n",
    "    if each_hashtag != 'null;':\n",
    "        for each in each_hashtag.strip().split(' '):\n",
    "            hashtags_count_dict[each] = hashtags_count_dict.get(each, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_len = []\n",
    "for each_hashtag in train_data['hashtags']:\n",
    "    if each_hashtag != 'null;':\n",
    "        hashtags_len.append(len(each_hashtag.split(' ')))\n",
    "    else:\n",
    "        hashtags_len.append(0)\n",
    "train_data['hashtags_len'] = hashtags_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_value= []\n",
    "for each_hashtag in train_data['hashtags']:\n",
    "    if each_hashtag != 'null;':\n",
    "        tmp = 0\n",
    "        for each in each_hashtag.strip().split(' '):\n",
    "            tmp += hashtags_count_dict[each]\n",
    "        hashtags_value.append(tmp)\n",
    "    else:\n",
    "        hashtags_value.append(0)\n",
    "train_data['hashtags_value'] = hashtags_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saleprice correlation matrix\n",
    "corrmat = train_data[['solution', 'hashtags_len', 'hashtags_value']].corr()\n",
    "k = 10 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'solution')['solution'].index\n",
    "cm = np.corrcoef(train_data[cols].values.T)\n",
    "f, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['urls_count'] = train_data['urls'].map(lambda x : 1 if x != 'null;' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_mapping = {label:idx for idx,label in enumerate(set(train_data['entities']))}\n",
    "# train_data['entities'] = train_data['entities'].map(class_mapping)\n",
    "# class_mapping = {label:idx for idx,label in enumerate(set(train_data['sentiment']))}\n",
    "# train_data['sentiment'] = train_data['sentiment'].map(class_mapping)\n",
    "# class_mapping = {label:idx for idx,label in enumerate(set(train_data['mentions']))}\n",
    "# train_data['mentions'] = train_data['mentions'].map(class_mapping)\n",
    "# class_mapping = {label:idx for idx,label in enumerate(set(train_data['hashtags']))}\n",
    "# train_data['hashtags'] = train_data['hashtags'].map(class_mapping)\n",
    "# class_mapping = {label:idx for idx,label in enumerate(set(train_data['urls']))}\n",
    "# train_data['urls'] = train_data['urls'].map(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['solution','#followers','#friends','#favorites']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % train_data['solution'].skew())\n",
    "print(\"Kurtosis: %f\" % train_data['solution'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(train_data['solution']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_data.loc[train_data['solution'] < 30]['solution']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_data[train_data['#favorites'] == 30]['solution'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "难以根据日期来判断solution的结果，并且solution存在大量的异常点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'timestamp'\n",
    "data = pd.concat([train_data['solution'], train_data[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='solution', ylim=(0,20));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'weekday'\n",
    "data = pd.concat([train_data['solution'], train_data[var]], axis=1)\n",
    "#data.plot.scatter(x=var, y='solution', ylim=(0,300000));\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=var, y=\"solution\", data=data)\n",
    "fig.axis(ymin=0, ymax=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看上去9月的solution更集中，但是9月的转发远少于10月"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "var = 'month'\n",
    "data = pd.concat([train_data['solution'], train_data[var]], axis=1)\n",
    "#data.plot.scatter(x=var, y='solution', ylim=(0,300000));\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=var, y=\"solution\", data=data)\n",
    "fig.axis(ymin=0, ymax=30);\n",
    "print(train_data[var].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'day'\n",
    "data = pd.concat([train_data['solution'], train_data[var]], axis=1)\n",
    "#data.plot.scatter(x=var, y='solution', ylim=(0,300000));\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=var, y=\"solution\", data=data)\n",
    "fig.axis(ymin=0, ymax=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'hour'\n",
    "data = pd.concat([train_data['solution'], train_data[var]], axis=1)\n",
    "#data.plot.scatter(x=var, y='solution', ylim=(0,300000));\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=var, y=\"solution\", data=data)\n",
    "fig.axis(ymin=0, ymax=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentiment不该如此简单的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'sentiment'\n",
    "print(train_data[var].value_counts())\n",
    "data = pd.concat([train_data['solution'], train_data[var]], axis=1)\n",
    "#data.plot.scatter(x=var, y='solution', ylim=(0,300000));\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=var, y=\"solution\", data=data)\n",
    "fig.axis(ymin=0, ymax=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'entities'\n",
    "data = pd.concat([train_data['solution'], train_data[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='solution', ylim=(0,50000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'mentions'\n",
    "data = pd.concat([train_data['solution'], train_data[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='solution', ylim=(0,300000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'hashtags'\n",
    "data = pd.concat([train_data['solution'], train_data[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='solution', ylim=(0,50000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = '#followers'\n",
    "data = pd.concat([train_data['solution'], train_data[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='solution', ylim=(0,50000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = '#friends'\n",
    "data = pd.concat([train_data['solution'], train_data[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='solution', ylim=(0,50000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data[(train_data['#friends']<=4000000)&(train_data['#friends']>=2000000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = '#favorites'\n",
    "data = pd.concat([train_data['solution'], train_data[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='solution', xlim=(0,10000), ylim=(0,10000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix\n",
    "corrmat = train_data.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "train_data['#friends_new']=train_data['#friends'].map(lambda x: 4486562-x)\n",
    "train_data['#followers_new']=train_data['#followers'].map(lambda x: 1166612-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saleprice correlation matrix\n",
    "k = 10 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'solution')['solution'].index\n",
    "cm = np.corrcoef(train_data[cols].values.T)\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram and normal probability plot\n",
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "sns.distplot(train_data['solution'], fit=norm);\n",
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "res = stats.probplot(train_data['solution'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying log transformation\n",
    "train_data['solution'] = np.log(1 + train_data['solution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram and normal probability plot\n",
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "sns.distplot(train_data['solution'], fit=norm);\n",
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "res = stats.probplot(train_data['solution'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram and normal probability plot\n",
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "sns.distplot(train_data['#favorites'], fit=norm);\n",
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "res = stats.probplot(train_data['#favorites'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data transformation\n",
    "train_data['#favorites'] = np.log(1 + train_data['#favorites'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram and normal probability plot\n",
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "sns.distplot(train_data['#favorites'], fit=norm);\n",
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "res = stats.probplot(train_data['#favorites'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram and normal probability plot\n",
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "sns.distplot(train_data['#friends'], fit=norm);\n",
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "res = stats.probplot(train_data['#friends'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['#friends'] = np.log(1 + train_data['#friends'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram and normal probability plot\n",
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "sns.distplot(train_data['#friends'], fit=norm);\n",
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "res = stats.probplot(train_data['#friends'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram and normal probability plot\n",
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "sns.distplot(train_data['#followers'], fit=norm);\n",
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "res = stats.probplot(train_data['#followers'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['#followers'] = np.log(1 + train_data['#followers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram and normal probability plot\n",
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "sns.distplot(train_data['#followers'], fit=norm);\n",
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "res = stats.probplot(train_data['#followers'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saleprice correlation matrix\n",
    "k = 10 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'solution')['solution'].index\n",
    "cm = np.corrcoef(train_data[cols].values.T)\n",
    "f, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = '#followers'\n",
    "data = pd.concat([train_data['solution'], train_data[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='solution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = '#friends'\n",
    "data = pd.concat([train_data['solution'], train_data[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='solution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(train_data['#friends'], train_data['solution'], lowess=True, color=\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_feature = ['timestamp', '#followers', '#friends',\n",
    "       '#favorites', 'weekday', 'month', 'day', 'hour', 'year', 'sentiment_left',\n",
    "       'sentiment_right', 'sentiment_sum', 'sentiment_sumabs',\n",
    "       'sentiment_diff', 'sentiment_div', 'entities_mean', 'entities_max',\n",
    "       'entities_min', 'entities_std', 'entities_len', 'mentions_len',\n",
    "       'mentions_value', 'hashtags_len', 'hashtags_value', 'urls_count', 'solution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_now = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-07-25_15-07-34'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
